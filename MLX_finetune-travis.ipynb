{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning DialoGPT with Clean Separation of Training and Inference\n",
    "\n",
    "This notebook demonstrates fine-tuning DialoGPT on a comments dataset with a clean separation between:\n",
    "1. Training phase\n",
    "2. Saving the model\n",
    "3. Loading the model from disk\n",
    "4. Inference phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from torch.optim import AdamW\n",
    "import os\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# Define device as MPS if available (for Mac with Apple Silicon)\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is built: True\n"
     ]
    }
   ],
   "source": [
    "# Verify MPS is built and available\n",
    "if device.type == 'mps':\n",
    "    print(f\"MPS is built: {torch.backends.mps.is_built()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>label</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fantastic effort!</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>Thank you, that means the world to me!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fantastic effort!</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>I appreciate your kind words!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>This is the worst thing I've seen.</td>\n",
       "      <td>troll</td>\n",
       "      <td>Apologies if this wasn't up to the mark.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I really appreciate your hard work!</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>Thank you, that means the world to me!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pathetic!</td>\n",
       "      <td>troll</td>\n",
       "      <td>I'm always open to constructive criticism.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>You should just quit.</td>\n",
       "      <td>troll</td>\n",
       "      <td>I respect your opinion, and I'll keep improving.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I've seen better work from a 5-year-old.</td>\n",
       "      <td>troll</td>\n",
       "      <td>I apologize if this didn't meet your expectati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Brilliant execution!</td>\n",
       "      <td>appreciation</td>\n",
       "      <td>Thanks a lot!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Terrible effort!</td>\n",
       "      <td>troll</td>\n",
       "      <td>I respect your opinion, and I'll keep improving.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>This is a complete waste of time.</td>\n",
       "      <td>troll</td>\n",
       "      <td>I apologize if this didn't meet your expectati...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    comment         label  \\\n",
       "0                         Fantastic effort!  appreciation   \n",
       "1                         Fantastic effort!  appreciation   \n",
       "2        This is the worst thing I've seen.         troll   \n",
       "3       I really appreciate your hard work!  appreciation   \n",
       "4                                 Pathetic!         troll   \n",
       "5                     You should just quit.         troll   \n",
       "6  I've seen better work from a 5-year-old.         troll   \n",
       "7                      Brilliant execution!  appreciation   \n",
       "8                          Terrible effort!         troll   \n",
       "9         This is a complete waste of time.         troll   \n",
       "\n",
       "                                               reply  \n",
       "0             Thank you, that means the world to me!  \n",
       "1                      I appreciate your kind words!  \n",
       "2           Apologies if this wasn't up to the mark.  \n",
       "3             Thank you, that means the world to me!  \n",
       "4         I'm always open to constructive criticism.  \n",
       "5   I respect your opinion, and I'll keep improving.  \n",
       "6  I apologize if this didn't meet your expectati...  \n",
       "7                                      Thanks a lot!  \n",
       "8   I respect your opinion, and I'll keep improving.  \n",
       "9  I apologize if this didn't meet your expectati...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset from CSV file\n",
    "data = pd.read_csv('comments_dataset.csv')\n",
    "comments = data['comment'].tolist()\n",
    "labels = data['label'].tolist()\n",
    "replies = data['reply'].tolist()\n",
    "\n",
    "# Display the first few rows\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class                                                                                               \n",
    "class CommentsDataset(Dataset):                                                                                      \n",
    "    def __init__(self, data, comments, replies, labels, tokenizer, max_length=128):                                  \n",
    "        self.comments = comments                                                                                     \n",
    "        self.replies = replies                                                                                       \n",
    "        self.labels = labels                                                                                         \n",
    "        self.tokenizer = tokenizer                                                                                   \n",
    "        self.max_length = max_length                                                                                 \n",
    "                                                                                                                    \n",
    "    def __len__(self):                                                                                               \n",
    "        return len(self.comments)                                                                                    \n",
    "                                                                                                                    \n",
    "    def __getitem__(self, idx):                                                                                      \n",
    "        # Format as a conversation with a clear separator                                                            \n",
    "        conversation = f\"User: {self.comments[idx]}\\nAssistant: {self.replies[idx]}\"                                 \n",
    "                                                                                                                    \n",
    "        # Tokenize the entire conversation                                                                           \n",
    "        encoding = self.tokenizer(                                                                                   \n",
    "            conversation,                                                                                            \n",
    "            padding=\"max_length\",                                                                                    \n",
    "            truncation=True,                                                                                         \n",
    "            max_length=self.max_length,                                                                              \n",
    "            return_tensors='pt'                                                                                      \n",
    "        )                                                                                                            \n",
    "                                                                                                                    \n",
    "        # For training, we need the labels to be the same as input_ids                                               \n",
    "        # This is for the causal language modeling objective                                                         \n",
    "        input_ids = encoding['input_ids'].squeeze()                                                                  \n",
    "        attention_mask = encoding['attention_mask'].squeeze()                                                        \n",
    "        labels = input_ids.clone()                                                                                   \n",
    "                                                                                                                    \n",
    "        return {                                                                                                     \n",
    "            'input_ids': input_ids,                                                                                  \n",
    "            'attention_mask': attention_mask,                                                                        \n",
    "            'labels': labels                                                                                         \n",
    "        } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Base Model for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base DialoGPT-small model...\n"
     ]
    }
   ],
   "source": [
    "# Load the base model and tokenizer\n",
    "print(\"Loading base DialoGPT-small model...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\n",
    "model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-small').to(device)\n",
    "\n",
    "# Set padding token\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "dataset = CommentsDataset(data, comments, replies, labels, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 1/1 Loss: 0.04108741134405136\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "num_epochs = 1\n",
    "learning_rate = 2e-5\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        # Move batch data to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Save the Fine-tuned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving fine-tuned model to ./fine_tuned_dialoGPT...\n",
      "Model and tokenizer saved successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create directory for the model if it doesn't exist\n",
    "save_dir = \"./fine_tuned_dialoGPT\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(f\"Saving fine-tuned model to {save_dir}...\")\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(\"Model and tokenizer saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Clean Separation: Clear Model from Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing model from memory...\n",
      "Model cleared from memory.\n"
     ]
    }
   ],
   "source": [
    "# Clear the model from memory to ensure clean separation\n",
    "print(\"Clearing model from memory...\")\n",
    "del model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "print(\"Model cleared from memory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load the Fine-tuned Model from Disk for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model from ./fine_tuned_dialoGPT...\n",
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model from disk\n",
    "print(f\"Loading fine-tuned model from {save_dir}...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_dir)\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reply(comment):                                                                                         \n",
    "     \"\"\"                                                                                                              \n",
    "     Generate a reply using the fine-tuned model loaded from disk.                                                    \n",
    "     This function uses the model that was explicitly loaded from ./fine_tuned_dialoGPT                               \n",
    "     \"\"\"                                                                                                              \n",
    "     # Format the input as a conversation prompt                                                                      \n",
    "     prompt = f\"User: {comment}\\nAssistant:\"                                                                          \n",
    "                                                                                                                      \n",
    "     inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True)                                                    \n",
    "     input_ids = inputs['input_ids'].to(device)                                                                       \n",
    "     attention_mask = inputs['attention_mask'].to(device)                                                             \n",
    "                                                                                                                      \n",
    "     # Generate reply                                                                                                 \n",
    "     reply_ids = model.generate(                                                                                      \n",
    "         input_ids=input_ids,                                                                                         \n",
    "         attention_mask=attention_mask,                                                                               \n",
    "         max_length=len(input_ids[0]) + 50,  # Allow for a reasonable response length                                 \n",
    "         pad_token_id=tokenizer.eos_token_id,                                                                         \n",
    "         temperature=0.7,                                                                                             \n",
    "         top_k=50,                                                                                                    \n",
    "         top_p=0.9,                                                                                                   \n",
    "         do_sample=True                                                                                               \n",
    "     )                                                                                                                \n",
    "                                                                                                                      \n",
    "     # Decode only the generated part, not the input prompt                                                           \n",
    "     generated_text = tokenizer.decode(reply_ids[0], skip_special_tokens=True)                                        \n",
    "                                                                                                                      \n",
    "     # Extract only the Assistant's reply                                                                             \n",
    "     reply = generated_text.split(\"Assistant:\")[-1].strip()                                                           \n",
    "                                                                                                                      \n",
    "     # If the reply is empty, return a default response                                                               \n",
    "     if not reply:                                                                                                    \n",
    "         reply = \"I appreciate your comment.\"                                                                         \n",
    "                                                                                                                      \n",
    "     return reply "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test the Model with Sample Comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comment: Your work is amazing!\n",
      "Reply: Thank you, that means the world to me!\n",
      "\n",
      "Comment: I really appreciate your effort\n",
      "Reply: I appreciate your kind words!\n",
      "\n",
      "Comment: This is terrible\n",
      "Reply: I'm sorry you feel that way.\n",
      "\n",
      "Comment: Nice work!\n",
      "Reply: Really appreciate it!\n",
      "\n",
      "Comment: your work is good?\n",
      "Reply: Thanks a ton!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test comments\n",
    "test_comments = [\n",
    "    \"Your work is amazing!\",\n",
    "    \"I really appreciate your effort\",\n",
    "    \"This is terrible\",\n",
    "    \"Nice work!\",\n",
    "    \"your work is good?\"\n",
    "]\n",
    "\n",
    "# Generate replies for each test comment\n",
    "for comment in test_comments:\n",
    "    reply = generate_reply(comment)\n",
    "    print(f\"Comment: {comment}\")\n",
    "    print(f\"Reply: {reply}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\"\"\"Data Loader : While training a model, we typically want to pass samples in “minibatches”, \n",
    "reshuffle the data at every epoch to reduce model overfitting, \n",
    "xand use Python’s multiprocessing to speed up data retrieval.\"\"\"\n",
    "\n",
    "# Custom dataset class\n",
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, data, comments, replies, label, tokenizer, max_length=128):\n",
    "        self.comments = comments\n",
    "        self.replies = replies\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize input (comment) and target (reply)\n",
    "        inputs = self.tokenizer(\n",
    "            self.comments[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = self.tokenizer(\n",
    "            self.label[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            self.replies[idx], #Using iloc because we are using pandas DataFrame\n",
    "            padding=\"max_length\", #Using padding to makesure all the input sequences have the same length\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        #Many PyTorch models, especially transformer-based models, \n",
    "        # expect input tensors to have specific shapes. If the tensors have unnecessary dimensions, it can lead to errors.\n",
    "        # Correct names: `input_ids` and `attention_mask`\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_s = label['input_ids'].squeeze()\n",
    "        labels = labels['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label__s': label_s,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Sample data for testing\n",
    "comments = [\"This tutorial was helpful!\", \"Can you explain step 3?\"]\n",
    "replies = [\"Glad it helped!\", \"Sure, let me explain step 3 in detail.\"]\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = CommentsDataset(data, comments, label, replies, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\"\"\"Data Loader : While training a model, we typically want to pass samples in “minibatches”, \n",
    "reshuffle the data at every epoch to reduce model overfitting, \n",
    "xand use Python’s multiprocessing to speed up data retrieval.\"\"\"\n",
    "\n",
    "# Custom dataset class\n",
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, data, comments, replies, label, tokenizer, max_length=128):\n",
    "        self.comments = comments\n",
    "        self.replies = replies\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize input (comment) and target (reply)\n",
    "        inputs = self.tokenizer(\n",
    "            self.comments[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = self.tokenizer(\n",
    "            self.label[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            self.replies[idx], #Using iloc because we are using pandas DataFrame\n",
    "            padding=\"max_length\", #Using padding to makesure all the input sequences have the same length\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        #Many PyTorch models, especially transformer-based models, \n",
    "        # expect input tensors to have specific shapes. If the tensors have unnecessary dimensions, it can lead to errors.\n",
    "        # Correct names: `input_ids` and `attention_mask`\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_s = label['input_ids'].squeeze()\n",
    "        labels = labels['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label__s': label_s,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Sample data for testing\n",
    "comments = [\"This tutorial was helpful!\", \"Can you explain step 3?\"]\n",
    "replies = [\"Glad it helped!\", \"Sure, let me explain step 3 in detail.\"]\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = CommentsDataset(data, comments, label, replies, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\"\"\"Data Loader : While training a model, we typically want to pass samples in “minibatches”, \n",
    "reshuffle the data at every epoch to reduce model overfitting, \n",
    "xand use Python’s multiprocessing to speed up data retrieval.\"\"\"\n",
    "\n",
    "# Custom dataset class\n",
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, data, comments, replies, label, tokenizer, max_length=128):\n",
    "        self.comments = comments\n",
    "        self.replies = replies\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize input (comment) and target (reply)\n",
    "        inputs = self.tokenizer(\n",
    "            self.comments[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = self.tokenizer(\n",
    "            self.label[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            self.replies[idx], #Using iloc because we are using pandas DataFrame\n",
    "            padding=\"max_length\", #Using padding to makesure all the input sequences have the same length\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        #Many PyTorch models, especially transformer-based models, \n",
    "        # expect input tensors to have specific shapes. If the tensors have unnecessary dimensions, it can lead to errors.\n",
    "        # Correct names: `input_ids` and `attention_mask`\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_s = label['input_ids'].squeeze()\n",
    "        labels = labels['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label__s': label_s,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Sample data for testing\n",
    "comments = [\"This tutorial was helpful!\", \"Can you explain step 3?\"]\n",
    "replies = [\"Glad it helped!\", \"Sure, let me explain step 3 in detail.\"]\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = CommentsDataset(data, comments, label, replies, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\"\"\"Data Loader : While training a model, we typically want to pass samples in “minibatches”, \n",
    "reshuffle the data at every epoch to reduce model overfitting, \n",
    "xand use Python’s multiprocessing to speed up data retrieval.\"\"\"\n",
    "\n",
    "# Custom dataset class\n",
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, data, comments, replies, label, tokenizer, max_length=128):\n",
    "        self.comments = comments\n",
    "        self.replies = replies\n",
    "        self.label = label\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize input (comment) and target (reply)\n",
    "        inputs = self.tokenizer(\n",
    "            self.comments[idx],\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        label = self.tokenizer(\n",
    "            self.label[idx],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        labels = self.tokenizer(\n",
    "            self.replies[idx], #Using iloc because we are using pandas DataFrame\n",
    "            padding=\"max_length\", #Using padding to makesure all the input sequences have the same length\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        #Many PyTorch models, especially transformer-based models, \n",
    "        # expect input tensors to have specific shapes. If the tensors have unnecessary dimensions, it can lead to errors.\n",
    "        # Correct names: `input_ids` and `attention_mask`\n",
    "        input_ids = inputs['input_ids'].squeeze()\n",
    "        attention_mask = inputs['attention_mask'].squeeze()\n",
    "        label_s = label['input_ids'].squeeze()\n",
    "        labels = labels['input_ids'].squeeze()\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'label__s': label_s,\n",
    "            'labels': labels\n",
    "        }\n",
    "\n",
    "# Sample data for testing\n",
    "comments = [\"This tutorial was helpful!\", \"Can you explain step 3?\"]\n",
    "replies = [\"Glad it helped!\", \"Sure, let me explain step 3 in detail.\"]\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "dataset = CommentsDataset(data, comments, label, replies, tokenizer)\n",
    "train_loader = DataLoader(dataset, batch_size=4, shuffle=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
